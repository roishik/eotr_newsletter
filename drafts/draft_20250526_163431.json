{
    "overall_prompt": "What are you doing?:\nYou are writing a section inside a newsletter about autonomous vehicles, the automotive industry, and AI technology. Only write about the relevant content of this section - This text will be a part of the larger newsletter (no need for welcome notes). Avoid AI chatbot introductions, such as 'here is the response to your request'.\nWriting style:\nWrite in a dynamic, conversational, and friendly tone, as if speaking directly to the reader. Keep the language approachable but insightful, mixing professional analysis with a sense of curiosity and enthusiasm. Use simple, clear sentences, but don't shy away from technical terms when necessary\u2014just explain them naturally and without overcomplication. Add thoughtful commentary that connects news or updates to broader implications, offering personal insights or lessons. Maintain an optimistic and forward-thinking voice, encouraging readers to reflect and engage while keeping the overall mood warm and encouraging. Don't be too optimistic and avoid making announcements that are bigger than the actual news.\nLength:\nKeep the response concise and focused on the key points.\nWhat to write about?\nOffer a new lens on the news, providing a fresh perspective or a unique angle that doubts the status quo or offers a new way of thinking.",
    "windshield_urls": "",
    "windshield_notes": "",
    "windshield_prompt": "Summarize the articles in 2\u20133 concise paragraphs focusing on their relevance to the autonomous vehicle and AI industry. Please be succinct and avoid unnecessary details. Write in first-person singular.",
    "num_rearview": 2,
    "dashboard_urls": "https://www.statista.com/chart/13450/perceived-safety-of-self-driving-cars/",
    "dashboard_notes": "tell the story of the Xiaomi car detailed here:\n\"\"\"\nThe Xiaomi SU7 Highway Crash: What Happened, Why It Matters, and What Comes Next?\nPosted by:\n\nENERGYDM Group\n\n|\n\nOn:\n\nMay 23, 2025\n|\n\n\n1  Accident Overview\nOn 29 March 2025, a base-trim Xiaomi SU7 travelling on the Dezhou\u2013Shangrao Expressway in Anhui, China, struck a roadside concrete barrier while Navigate on Autopilot (NOA) was active. System logs show the car was cruising at 116 km h-1 when the NOA issued an obstacle alert; the driver grabbed the wheel one second later, but the sedan still hit the barrier at \u224897 km h-1, burst into flames, and all three occupants were killed [1]. Xiaomi told police it is co-operating fully and has already supplied raw driving data for download [2].\n\n2  Crash Timeline and Key Telemetry\nLocal time (hh:mm:ss)\tEvent\tVehicle state\n22 : 27 : 17\tNOA engaged\t116 km h-1\n22 : 28 : 17\tMild-distraction warning\t\u2014\n22 : 36 : 48\t\u201cHands on wheel\u201d alert\t\u2014\n22 : 44 : 24\tObstacle ahead alert\tBrake request\n22 : 44 : 25\tDriver takes control\tWheel \u2248 22\u00b0 L, brake 31 %\n22 : 44 : 26\tImpact with barrier\t97 km h-1\n22 : 44 : 28\teCall triggered\t\u2014\n(The sequence is reconstructed from Xiaomi\u2019s own event log [3] and highway-patrol data [1]).\nThe human had \u22480.8 s from alert to impact\u2014far below the 2.3 s average NHTSA reaction benchmark. AEB logic slowed the car ~19 km h-1 but did not fully stop because the software treats rigid, stationary objects differently from moving lead vehicles [1].\n\n3  Technical Factors\nSensor suite (base SU7) \u2014 11 cameras, 3 mm-wave radars, 12 ultrasonics; no LiDAR [6].\nCompute \u2014 Single NVIDIA DRIVE Orin-X SoC (254 TOPS) running Xiaomi Pilot (L2+) software.\nBattery \u2014 73.6 kWh BYD FinDreams LFP \u201cblade\u201d pack (400 V) [8].\nBrakes & chassis \u2014 Brembo four-piston front calipers, Bosch ABS/ESC, ZF dampers [9] , [10] .\nWhy did AEB not save the car?  Xiaomi\u2019s driver-assist logic prioritises moving obstacles; at highway speeds the camera\u2013radar fusion often filters out static objects to avoid false positives. With only vision and radar, the barrier\u2019s geometry may have been mis-classified until it filled the field of view, leaving too little braking distance [13].\n\n4  Camera-Only vs LiDAR Debate\nHigher SU7 trims (Pro/Max) add a roof-mounted Hesai AT128 LiDAR and a second Orin for 508 TOPS of compute [7]. LiDAR gives centimetre-grade range and detects low-contrast obstacles\u2014even at night or in glare\u2014that cameras struggle with. Tesla\u2019s all-camera Autopilot faces similar scrutiny: the U.S. NHTSA is probing 2.4 million Teslas after multiple stationary-object crashes [11], while a 2023 recall addressed Autopilot misuse when hitting parked emergency vehicles [12]. Critics argue the Xiaomi accident shows why redundant sensing remains prudent until full-stack vision AI matures [13].\n\n5  Global Safety & Validation Standards\nChina\u2014after the crash, MIIT banned terms like \u201csmart\u201d or \u201cautonomous driving\u201d in ads and now requires type-approval for over-the-air (OTA) driving-software updates [17].\nUnited States\u2014NHTSA is adding four ADAS test items to NCAP and is openly investigating Tesla FSD performance in low-visibility scenarios [15], [11].\nEuropean Union\u2014Euro NCAP\u2019s 2024 protocol roughly doubles the number of assisted-driving scenarios and emphasises stationary-object AEB performance [14].\nThese tighter test matrices contrast with China\u2019s rapid \u201crelease-first, refine-later\u201d cycle, where new entrants like Xiaomi shipped the SU7 in barely two years.\n\n6  Market Fallout\nWithin two weeks of the crash, new SU7 orders fell \u224855 % month-on-month as refunds and social-media backlash mounted [4]. Analysts fear the incident could slow Chinese EV expansion abroad just as the EU weighs tariffs on subsidised imports [18]. Meanwhile, LiDAR suppliers such as Hesai expect the tragedy to accelerate multi-sensor demand and are already planning cheaper next-gen units to reach mass-market price points [7].\n\n\n7  How the SU7 Compares\nMetric\tXiaomi SU7 (base)\tTesla Model 3 (2025)\tNIO ET5\tEU Benchmark (Euro NCAP \u201824)\nSensors\tCamera + radar\tCamera only\tCamera + LiDAR + radar\tCamera + LiDAR + radar (test spec)\nAEB stationary target (110 km h-1)\tPartial stop (software-limited)\tOften no braking\tFull stop\tMust stop to score \u201cVery Good\u201d\nDevelopment cycle\t\u224824 mo\t\u224818 mo refresh\t\u224836 mo\t48 \u2013 60 mo typical\nRecent fatal ADAS crash?\tYes (2025)\tYes (2023, 2024)\tYes (2021)\t\u2014\n8  Take-Aways for Engineers & Enthusiasts\nRedundancy matters\u2014A single perception modality can miss edge-cases. Multi-sensor stacks cost more but mitigate blind-spots.\nHuman factors remain critical\u2014Even the quickest alerts are futile without enough reaction time; driver-monitoring must escalate earlier.\nValidation is no longer optional\u2014Euro NCAP\u2019s 40 new tests and MIIT\u2019s OTA-approval rule show regulators aligning safety with software pace.\nMarket perception can swing overnight\u2014A single high-profile crash cut SU7 demand in half and sparked nationwide ad-language reform.\n9  Conclusion\nThe March 29 Xiaomi SU7 tragedy is a stark reminder that today\u2019s Level-2 \u201cpilot\u201d features are assistance, not autonomy. Vision-heavy systems can work brilliantly in most conditions, but the physics of a 2-tonne sedan give drivers and algorithms mere fractions of a second to react when things go wrong. As China tightens oversight and Europe and the U.S. expand ADAS testing, EV makers\u2014new and established\u2014must balance speed-to-market with exhaustive safety validation. Until then, the smartest car on the road is still the one with an alert human behind the wheel.\n\"\"\"\nthen in the why it matters focus on the public perception of AV safety. then what i think is that it is hard to develop such a huge tech (autonomous driving) when each false (=crash) can destroy the company reputation. we should really take it step by step.",
    "dashboard_prompt": "Write 3 parts:\n- What's New: Describe key trends or insights concisely.\n- Why It Matters: Explain the impact on the industry succinctly.\n- What I Think: Share a brief personal opinion.",
    "nextlane_urls": "https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/",
    "nextlane_notes": "google released a new video generation model that shows groundbraking results. you must take a look of some of the results (here I'll add some links). Are we approaching for text-to-ai driving simulation? or is it still far away?",
    "nextlane_prompt": "Summarize academic news in 2\u20133 concise paragraphs, highlighting its implications for the ai industry. Keep it brief and to the point.",
    "generated_sections": {
        "Windshield View": "**The Anti-Tesla Strategy: What If Simplicity Actually Wins?**\n\nMeet Slate Auto\u2014a startup that's building electric trucks by deliberately doing the opposite of what every other EV company is doing. While the industry races toward autonomous features and premium tech, Slate is stripping everything down to create a sub-$20,000 electric truck with manual windows, no self-driving capabilities, and a focus on DIY customization. Think of them as the anti-Tesla: affordable first, complexity never.\n\nI find myself questioning whether this \"less is more\" approach might actually be the smarter path forward in today's EV landscape. While Tesla started with luxury and worked its way down, Slate is diving straight into the affordable market with a truck that deliberately strips away the autonomous features and tech complexity that have driven EV prices skyward. It's a fascinating inversion of the traditional startup playbook\u2014instead of promising self-driving futures, they're betting on manual windows and DIY customization.\n\nWhat strikes me most is how this reflects a growing fatigue with the \"autonomy at any cost\" mentality that's dominated the industry. Slate's CEO hit on something real when he said the focus on autonomous technology has priced out most Americans. While companies like Waymo and Tesla continue pouring billions into self-driving capabilities, Slate is essentially asking: what if we just made cars people can actually afford first? The timing feels particularly sharp given recent recalls affecting autonomous systems\u2014Zoox just issued its second robotaxi software recall this month alone.\n\nThe customization angle also suggests a different relationship between humans and their vehicles\u2014one where you're actively involved in building and modifying your truck rather than passively consuming an AI-driven experience. It's almost nostalgic, but in a way that might be exactly what the market needs. Whether this bare-bones approach can scale profitably remains the big question, but I can't help wondering if Slate has identified something the rest of the industry missed in its rush toward autonomous everything.",
        "Rearview Mirror 1": "**Meta's 3D Asset Generator: The Metaverse's Manufacturing Revolution**\n\nMeta's AssetGen 2.0 can transform text prompts into production-ready 3D objects, potentially shifting the metaverse from a content scarcity problem to a content curation challenge. While democratizing 3D creation sounds transformative, the real test will be whether this floods virtual worlds with generic assets or genuinely empowers unique creative expression.",
        "Rearview Mirror 2": "**The $1.3 Billion Talent Grab That Has Everyone Talking**\n\nAmazon's massive acquisition of Adept AI for $1.3 billion\u2014primarily to poach its entire team while leaving the actual company behind\u2014signals that in the AI arms race, human expertise has become more valuable than the technology itself.",
        "Dashboard Data": "**What's New:**\n\nIn a dramatic and tragic turn of events, a Xiaomi SU7 equipped with Navigate on Autopilot (NOA) collided with a roadside barrier on the Dezhou\u2013Shangrao Expressway in China, resulting in the loss of three lives. This incident has reignited the debate over the reliance on camera-only systems versus the inclusion of LiDAR in autonomous vehicles. The crash, which occurred while the vehicle was on autopilot, highlights the limitations of current driver-assist technologies, especially in handling stationary objects. Interestingly, higher trims of the SU7 include LiDAR, which could have potentially avoided this tragedy.\n\n**Why It Matters:**\n\nPublic perception of autonomous vehicle (AV) safety is at a critical juncture. This crash serves as a sobering reminder that the promise of autonomous driving comes with significant risks. Trust in AV technology can be fragile, and high-profile incidents like this one can lead to a swift decline in consumer confidence, as evidenced by the 55% drop in new orders for the SU7. This situation underscores the urgent need for rigorous safety validation and the cautious advancement of AV technologies. As safety standards tighten globally, manufacturers must prioritize robust, multi-sensor systems to reassure the public and avoid jeopardizing the industry's growth.\n\n**What I Think:**\n\nDeveloping autonomous driving technology is a monumental task, and each misstep can have devastating consequences for a company's reputation. It's crucial that we approach this innovation with caution, ensuring each step forward is backed by comprehensive testing and validation. Racing to market without addressing potential safety gaps can be a recipe for disaster. Let's take it step by step, learning from each incident, and striving for a future where technology enhances safety rather than undermines it.",
        "The Next Lane": "**Google's Flow: A Creative Leap That Hints at Bigger Things**\n\nGoogle just dropped Flow, their new AI filmmaking tool that's basically VideoFX all grown up and professional. What's interesting isn't just the seamless transitions or cinematic quality\u2014it's how Google is positioning this as a creative enabler rather than a replacement tool. They're partnering with actual filmmakers, weaving their feedback into the product, and openly admitting they're \"still understanding the full potential.\" This approach feels refreshingly honest in a space where companies often oversell capabilities.\n\nBut here's where it gets intriguing for our world: if AI can now generate consistent characters across scenes, handle complex physics, and maintain cinematic realism, how far are we from text-to-driving simulations? The building blocks are there\u2014environmental consistency, realistic physics, object permanence. The gap between creating a convincing short film and generating a realistic driving scenario might be smaller than we think. While we're not quite at \"describe a rainy highway merge scenario and watch it unfold,\" Flow suggests we're moving beyond proof-of-concept into genuinely useful territory. The question isn't whether AI will reshape how we train autonomous systems, but how quickly these creative tools will cross over into practical applications."
    },
    "edited_sections": {},
    "selected_provider": "OpenAI",
    "selected_model": "gpt-4o",
    "language": "English",
    "theme": "Light",
    "rearview_urls_1": "",
    "rearview_notes_1": "Here is a META blogpost about a new model they released:\n\"\"\"\nIntroducing Meta 3D AssetGen 2.0: A new foundation model for 3D content creation\nBlog Hero Image\n\nBy: Rakesh Ranjan, Andrea Vedaldi, Mahima Gupta, Christopher Ocampo, Ocean Quigley\nToday, we\u2019re introducing Meta\u2019s latest 3D foundation model, AssetGen 2.0, which was designed to create high-quality 3D assets from text and image prompts. AssetGen 2.0 consists of two models developed within Meta: one to generate 3D meshes and another to generate textures. Building on its predecessor, AssetGen 1.0, our latest innovation represents a significant leap forward in 3D generative AI research, leveraging a single-stage 3D diffusion model to deliver 3D meshes with dramatically improved detail and fidelity, while our complementary TextureGen model ensures that these assets are not only visually stunning but also production-ready with high-quality textures. Our single-stage diffusion approach ensures that the 3D assets have geometric consistency with very fine details (see outputs below). Unlike AssetGen 1.0, AssetGen 2.0 uses 3D diffusion for geometry estimation and is trained on a large corpus of 3D assets. For texture generation, it also introduces new methods for improved view consistency, texture in-painting, and increased texture resolution. We\u2019re currently using AssetGen 2.0 internally for 3D worlds creation, and we will roll out later this year to Horizon creators.\n\n\n\n\n\n\nTextured meshes generated using AssetGen v2.\n\n\n\n\nAnimatable characters generated using AssetGen v2.\nWe expect AssetGen 2.0 to help democratize the creation of 3D experiences, making it as accessible as 2D content creation and opening up new creative possibilities for artists, designers, and developers working on Meta\u2019s Horizon and Avatar platforms. Further, in the coming months, AssetGen will serve as a foundational building block to enable auto-regressive generation of entire 3D scenes, enabling a step function change in creation for our users. This new model will be able to generate complex 3D environments by sequentially generating individual objects, structures, or elements using simple text or image prompts.\nEach asset in this scene was generated using AssetGen v2.\nMeta is committed to pushing the boundaries of what\u2019s possible in the field of generative AI. AssetGen 2.0 has been designed to deliver stunning visual quality, surpassing existing solutions and setting a new standard for the industry. As we continue to explore the vast potential of 3D generative AI, we\u2019re excited to see how creators will leverage AssetGen 2.0 in the future to bring their most ambitious ideas to life and redefine 3D content creation.\nJoin us on this exciting journey as we unlock the full potential of 3D GenAI and empower a new generation of creators to build, innovate, and inspire. We are hiring for multiple roles including Research Scientists and Engineers who can help us build this future.\n\"\"\"",
    "rearview_prompt_1": "Provide a brief headline (bolded text, not an actual headline) and a one-sentence summary. Keep the response extremely concise\u2014no more than 2 sentences total.",
    "rearview_urls_2": "https://www.npr.org/2025/05/22/nx-s1-5407548/openai-jony-ive-io-deal-ai-devices",
    "rearview_notes_2": "The highest aquihire ever?",
    "rearview_prompt_2": "Provide a brief headline (bolded text, not an actual headline) and a one-sentence summary. Keep the response extremely concise\u2014no more than 2 sentences total."
}